{
  "permissions": {
    "allow": [
      "Bash(gh issue list:*)",
      "Bash(git checkout:*)",
      "Bash(npm run build:*)",
      "Bash(git add:*)",
      "Bash(git commit -m \"$(cat <<''EOF''\nAdd Ollama local LLM server support\n\nThis commit implements support for locally hosted LLM servers using Ollama,\naddressing issue #1. Users can now run LUMEN analysis with local models\nfor enhanced privacy and cost savings.\n\nChanges:\n- Created new OllamaProvider implementing the LLMProvider interface\n- Added ''ollama'' to LLMProvider type union\n- Registered Ollama in provider loader and metadata registry\n- Updated LLMSettings UI to support endpoint configuration for providers\n  that don''t require API keys\n- Updated LLMAnalysis to pass endpoint configuration to provider\n- Ollama validates service availability instead of API key\n- Dynamic model loading from local Ollama server\n\nFeatures:\n- No API key required (runs locally)\n- Configurable endpoint (default: http://localhost:11434)\n- Automatic model discovery from local Ollama installation\n- Full integration with existing LUMEN analysis workflow\n- Optimized parameters for better output (num_ctx: 8192, etc.)\n\nTested with multiple models (phi, qwen2.5, mistral) and verified working.\n\nðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)\n\nCo-Authored-By: Claude <noreply@anthropic.com>\nEOF\n)\")",
      "Bash(git commit:*)"
    ],
    "deny": [],
    "ask": []
  }
}
